{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "import wandb\n",
    "\n",
    "# local imports\n",
    "from week1 import create_bag_visual_word, compute_feature_histogram\n",
    "from cnn_models import get_CNN_model\n",
    "from ml_models import get_ML_model\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU CHECK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow is not able to access the GPU !!\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print(\"Default GPU Device: {}\".format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Tensorflow is not able to access the GPU !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "IMG_CHANNEL = 3\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 8\n",
    "NUM_EPOCH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/ghome/group05/mcv/datasets/C3/MIT_large_train/train\"\n",
    "test_dir = \"/ghome/group05/mcv/datasets/C3/MIT_large_train/test\"\n",
    "validation_dir = \"/ghome/group05/mcv/datasets/C3/MIT_small_train_2/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET PREAPRATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,  # Degree range for random rotations\n",
    "    # width_shift_range=0.2,  # Range (as a fraction of total width) for horizontal shift\n",
    "    # height_shift_range=0.2,  # Range (as a fraction of total height) for vertical shift\n",
    "    # shear_range=0.2,  # Shear intensity (shear angle in counter-clockwise direction)\n",
    "    # zoom_range=0.2,  # Range for random zoom\n",
    "    horizontal_flip=True,  # Randomly flip inputs horizontally\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\",  # Strategy for filling in newly created pixels\n",
    "    rescale=1.0 / 255,  # normalize data\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # normalize data\n",
    ")\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # normalize data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, test and validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1881 images belonging to 8 classes.\n",
      "Found 807 images belonging to 8 classes.\n",
      "Found 2288 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=[\n",
    "        \"coast\",\n",
    "        \"forest\",\n",
    "        \"highway\",\n",
    "        \"inside_city\",\n",
    "        \"mountain\",\n",
    "        \"Opencountry\",\n",
    "        \"street\",\n",
    "        \"tallbuilding\",\n",
    "    ],\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=[\n",
    "        \"coast\",\n",
    "        \"forest\",\n",
    "        \"highway\",\n",
    "        \"inside_city\",\n",
    "        \"mountain\",\n",
    "        \"Opencountry\",\n",
    "        \"street\",\n",
    "        \"tallbuilding\",\n",
    "    ],\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "validation_dataset = validation_datagen.flow_from_directory(\n",
    "    directory=validation_dir,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=[\n",
    "        \"coast\",\n",
    "        \"forest\",\n",
    "        \"highway\",\n",
    "        \"inside_city\",\n",
    "        \"mountain\",\n",
    "        \"Opencountry\",\n",
    "        \"street\",\n",
    "        \"tallbuilding\",\n",
    "    ],\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    subset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = CustomEarlyStoping(monitor=\"val_accuracy\", threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get CNN feature vector and create feature and label for train and test\n",
    "def extract_features(model, dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for batch in tqdm(dataset, total=len(dataset)):\n",
    "      \n",
    "        images, batch_labels = batch\n",
    "\n",
    "        batch_features = model.predict_on_batch(images)\n",
    "        features.extend(batch_features)\n",
    "\n",
    "        labels.extend(batch_labels)\n",
    "        count += 1\n",
    "        if count == len(dataset):\n",
    "            break\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(images, labels, predictions):\n",
    "    # Set the number of images to display\n",
    "    num_images = min(len(images), 10)  # For example, display up to 10 images\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, num_images // 2, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(f\"True: {labels[i]}\\nPred: {predictions[i]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predicted_labels, test_features, test_labels, test_dataset):\n",
    "    # predictions = model.predict(test_features)\n",
    "    # predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_labels, predicted_labels)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"Actual Labels\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    print(classification_report(test_labels, predicted_labels))\n",
    "\n",
    "    # Plotting correctly and wrongly predicted images\n",
    "    correct = np.where(predicted_labels == test_labels)[0]\n",
    "    incorrect = np.where(predicted_labels != test_labels)[0]\n",
    "    plot_image_grid(\n",
    "        test_dataset[correct], test_labels[correct], predicted_labels[correct]\n",
    "    )\n",
    "    plot_image_grid(\n",
    "        test_dataset[incorrect], test_labels[incorrect], predicted_labels[incorrect]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    TRAINING_METHOD: str,\n",
    "    CNN_MODEL: str,\n",
    "    ML_cLASSIFIER_MODEL: str = None,\n",
    "    NUMER_PATCH: int = None,\n",
    "):\n",
    "    if TRAINING_METHOD == \"END_TO_END_CNN\":\n",
    "        model = get_CNN_model(\n",
    "            MODEL_NAME=CNN_MODEL,\n",
    "            IMG_SIZE=IMG_SIZE,\n",
    "            IMG_CHANNEL=IMG_CHANNEL,\n",
    "            NUM_CLASSES=NUM_CLASSES,\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=NUM_EPOCH,\n",
    "            validation_data=validation_dataset,\n",
    "            callbacks=[early_stopping_callback],\n",
    "        )\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "        return test_accuracy\n",
    "    elif TRAINING_METHOD == \"CNN_FEATURE_WITH_ML_CLASSIFIER\":\n",
    "        cnn_model = get_CNN_model(\n",
    "            MODEL_NAME=CNN_MODEL,\n",
    "            IMG_SIZE=IMG_SIZE,\n",
    "            IMG_CHANNEL=IMG_CHANNEL,\n",
    "            NUM_CLASSES=NUM_CLASSES,\n",
    "            ONLY_FEATURE=True,\n",
    "        )\n",
    "\n",
    "        train_features, train_labels = extract_features(cnn_model, train_dataset)\n",
    "        train_features = np.array(train_features)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "\n",
    "        test_features, test_labels = extract_features(cnn_model, train_dataset)\n",
    "        test_features = np.array(test_features)\n",
    "        test_labels = np.array(test_labels)\n",
    "        test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        ml_classifier = get_ML_model(\n",
    "            MODEL_NAME=ML_cLASSIFIER_MODEL, train_X=train_features, train_y=train_labels\n",
    "        )\n",
    "        test_accuracy = ml_classifier.score(test_features, test_labels)\n",
    "        \n",
    "        \n",
    "        test_pred = ml_classifier.predict(test_features)\n",
    "     \n",
    "\n",
    "        # 3. Create Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "        # 4. Get Class Indices to Names Map\n",
    "        class_indices_to_names = {v: k for k, v in test_dataset.class_indices.items()}\n",
    "\n",
    "        # 5. Adjust Confusion Matrix with Class Names\n",
    "        conf_matrix_with_names = pd.DataFrame(\n",
    "            conf_matrix,\n",
    "            index=[class_indices_to_names[i] for i in range(NUM_CLASSES)],\n",
    "            columns=[class_indices_to_names[i] for i in range(NUM_CLASSES)],\n",
    "        )\n",
    "\n",
    "        # 6. Plot Confusion Matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix_with_names, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        test_labels = [class_indices_to_names[i] for i in test_labels]\n",
    "        test_pred = [class_indices_to_names[i] for i in test_pred]\n",
    "        print(classification_report(test_labels, test_pred))\n",
    "\n",
    "      \n",
    "        return test_accuracy\n",
    "    elif TRAINING_METHOD == \"CNN_FEATURE_WITH_BOVW\":\n",
    "        cnn_model = get_CNN_model(\n",
    "            MODEL_NAME=CNN_MODEL,\n",
    "            IMG_SIZE=IMG_SIZE,\n",
    "            IMG_CHANNEL=IMG_CHANNEL,\n",
    "            NUM_CLASSES=NUM_CLASSES,\n",
    "            ONLY_FEATURE=True,\n",
    "        )\n",
    "\n",
    "        train_features, train_labels = extract_features(cnn_model, train_dataset)\n",
    "        train_features = np.array(train_features)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_labels = np.argmax(train_labels, axis=1)\n",
    "        print(f\"train_features shape: {train_features.shape}\")\n",
    "        print(f\"train_labels shape  : {train_labels.shape}\")\n",
    "\n",
    "        test_features, test_labels = extract_features(cnn_model, train_dataset)\n",
    "        test_features = np.array(test_features)\n",
    "        test_labels = np.array(test_labels)\n",
    "        test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "        print(f\"test_features shape: {test_features.shape}\")\n",
    "        print(f\"test_labels shape  : {test_labels.shape}\")\n",
    "\n",
    "        codebook = create_bag_visual_word(train_features, 5)\n",
    "\n",
    "        train_y = []\n",
    "        train_X = []\n",
    "\n",
    "        for features, label in zip(train_features, train_labels):\n",
    "            res = compute_feature_histogram(\n",
    "                features=features.reshape(1, -1), codebook=codebook\n",
    "            )\n",
    "\n",
    "            train_X.append(res)\n",
    "            train_y.append(label)\n",
    "\n",
    "        test_X = []\n",
    "        test_y = []\n",
    "\n",
    "        for features, label in zip(test_features, test_labels):\n",
    "            res = compute_feature_histogram(\n",
    "                features=features.reshape(1, -1), codebook=codebook\n",
    "            )\n",
    "\n",
    "            test_X.append(res)\n",
    "            test_y.append(label)\n",
    "\n",
    "        ml_classifier = get_ML_model(\n",
    "            MODEL_NAME=ML_cLASSIFIER_MODEL, train_X=train_X, train_y=train_y\n",
    "        )\n",
    "\n",
    "        test_accuracy = ml_classifier.score(test_X, test_y)\n",
    "        evaluate_model()\n",
    "        print(f\"ACCURACY: {test_accuracy}\")\n",
    "        return test_accuracy\n",
    "    elif TRAINING_METHOD == \"KEYPOINT_FEATURE_wITH_CNN\":\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optuna and weight & bias setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_kwargs = {\"project\": \"WEEK_3_FINDING_BEST_COMBINATIN\"}\n",
    "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_training_method = [\n",
    "    \"END_TO_END_CNN\",\n",
    "    \"CNN_FEATURE_WITH_ML_CLASSIFIER\",\n",
    "    \"CNN_FEATURE_WITH_BOVW\" \"KEYPOINT_FEATURE_wITH_CNN\",\n",
    "]\n",
    "available_CNN_models = [\n",
    "    \"pretrained_mobilenet_V2\",\n",
    "    \"pretrained_resnet50\",\n",
    "    \"pretrained_efficientnet\",\n",
    "    \"pretrained_vgg16\",\n",
    "    \"pretrained_vit\",\n",
    "    \"pretrained_densenet\",\n",
    "\n",
    "    \"scratch_CNN_Residual_Model\",\n",
    "    \"scratch_CNN_Patch_Model\",\n",
    "]\n",
    "available_ML_classifier_models = [\n",
    "    \"SVM\",\n",
    "    \"KNN\",\n",
    "    \"XGBOOST\",\n",
    "    \"DecisionTree\",\n",
    "    \"LogisticRegression\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    wandb.init(project=\"WEEK3_testing\")\n",
    "    selected_training_method = trial.suggest_categorical(\n",
    "        \"training_method\", available_training_method\n",
    "    )\n",
    "\n",
    "    selected_CNN_model = trial.suggest_categorical(\"CNN_model\", available_CNN_models)\n",
    "    if selected_CNN_model == \"CNN_Patch_Model\":\n",
    "        seleted_patch_size = trial.suggest_categorical(\n",
    "            \"patch_size\", [i - 1 for i in range(1, 100, 5)]\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        seleted_patch_size = None\n",
    "\n",
    "    if (\n",
    "        selected_training_method == \"CNN_FEATURE_WITH_ML_CLASSIFIER\"\n",
    "        or selected_training_method == \"CNN_FEATURE_WITH_BOVW\"\n",
    "    ):\n",
    "        selected_ML_classifier_model = trial.suggest_categorical(\n",
    "            \"ML_classifier_models\", available_ML_classifier_models\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        selected_ML_classifier_model = None\n",
    "\n",
    "    accuracy = train_test(\n",
    "        TRAINING_METHOD=selected_training_method,\n",
    "        CNN_MODEL=selected_CNN_model,\n",
    "        ML_cLASSIFIER_MODEL=selected_ML_classifier_model,\n",
    "        NUMER_PATCH = seleted_patch_size\n",
    "        \n",
    "    )\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "    )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"maximize\",\n",
    "#         storage=\"sqlite:///db.sqlite3\",\n",
    "#         pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "#     )\n",
    "#     study.optimize(\n",
    "#         objective,\n",
    "#         n_trials=1000,\n",
    "#         timeout=60000000000000000000,\n",
    "#         callbacks=[wandbc],  # weight and bias connection\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 6/118 [00:02<00:40,  2.78it/s]"
     ]
    }
   ],
   "source": [
    "TRAINING_METHOD = \"CNN_FEATURE_WITH_ML_CLASSIFIER\"\n",
    "CNN_MODEL = \"pretrained_mobilenet_V2\"\n",
    "ML_cLASSIFIER_MODEL = \"LogisticRegression\"\n",
    "NUMER_PATCH = 0\n",
    "\n",
    "\n",
    "accuracy = train_test(\n",
    "    TRAINING_METHOD=TRAINING_METHOD,\n",
    "    CNN_MODEL=CNN_MODEL,\n",
    "    ML_cLASSIFIER_MODEL=ML_cLASSIFIER_MODEL,\n",
    "    NUMER_PATCH=NUMER_PATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cvc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
